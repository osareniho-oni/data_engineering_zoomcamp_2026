{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31b0e21c-59ff-4d9f-af80-56fddb160c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6947a1-f919-4569-a202-f2de6e2c44ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/27 23:18:03 WARN Utils: Your hostname, Numinous, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/02/27 23:18:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/27 23:18:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('nyc-taxi') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c10aa3b-0c1b-441d-b425-bafeff783a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.1\n"
     ]
    }
   ],
   "source": [
    "# pysprk version\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fc1ba66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9f2730-f7e8-446e-82f4-0dc9e062f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Download the Parquet file for Yellow Taxi Trip Data from October 2024\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-11.parquet -O yellow_tripdata_2025-11.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec87ef5c-be5a-4cf7-a8e3-aa75c982cf4b",
   "metadata": {},
   "source": [
    "**Analysis of Partitioned Parquet File Sizes**\n",
    "This section examines the average size (in megabytes) of all files generated with the .parquet extension, focusing on determining which value most closely represents the typical file size among those created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f13c78f8-99e6-4b95-8ef8-ad0fa04a7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the downloaded Parquet file into a Spark DataFrame\n",
    "df_yellow = spark.read.parquet('yellow_tripdata_2025-11.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "225ee6d6-6bb1-4f28-a55d-f190cc449407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|       7| 2025-11-01 00:13:25|  2025-11-01 00:13:25|              1|         1.68|         1|                 N|          43|         186|           1|       14.9|  0.0|    0.5|       1.5|         0.0|                  1.0|       22.15|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-01 00:49:07|  2025-11-01 01:01:22|              1|         2.28|         1|                 N|         142|         237|           1|       14.2|  1.0|    0.5|      4.99|         0.0|                  1.0|       24.94|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-11-01 00:07:19|  2025-11-01 00:20:41|              0|          2.7|         1|                 N|         163|         238|           1|       15.6| 4.25|    0.5|      4.27|         0.0|                  1.0|       25.62|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-01 00:00:00|  2025-11-01 01:01:03|              3|        12.87|         1|                 N|         138|         261|           1|       66.7|  6.0|    0.5|       0.0|        6.94|                  1.0|       86.14|                 2.5|       1.75|              0.75|\n",
      "|       1| 2025-11-01 00:18:50|  2025-11-01 00:49:32|              0|          8.4|         1|                 N|         138|          37|           2|       39.4| 7.75|    0.5|       0.0|         0.0|                  1.0|       48.65|                 0.0|       1.75|               0.0|\n",
      "|       2| 2025-11-01 00:21:11|  2025-11-01 00:31:39|              1|         0.85|         1|                 N|          90|         100|           2|       10.7|  1.0|    0.5|       0.0|         0.0|                  1.0|       16.45|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-01 00:07:31|  2025-11-01 00:25:44|              1|         3.01|         1|                 N|         142|         170|           1|       19.1|  1.0|    0.5|       1.0|         0.0|                  1.0|       25.85|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-01 00:46:52|  2025-11-01 01:38:55|              3|         3.82|         1|                 N|         237|         144|           1|       42.2|  1.0|    0.5|      9.59|         0.0|                  1.0|       57.54|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-01 00:56:59|  2025-11-01 01:02:05|              1|         0.89|         1|                 N|         162|         161|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|       12.95|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-01 00:10:43|  2025-11-01 00:39:25|              3|         2.28|         1|                 N|         234|         162|           1|       24.0|  1.0|    0.5|      8.93|         0.0|                  1.0|       38.68|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-11-01 00:00:03|  2025-11-01 00:42:25|              2|          3.3|         1|                 N|         158|          88|           1|       35.9| 4.25|    0.5|      2.35|         0.0|                  1.0|        44.0|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-11-01 00:43:53|  2025-11-01 00:56:46|              2|          1.5|         1|                 N|          88|         148|           1|       12.8| 4.25|    0.5|       1.0|         0.0|                  1.0|       19.55|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-11-01 00:58:02|  2025-11-01 01:32:36|              2|          4.7|         1|                 N|         148|         236|           1|       32.4| 4.25|    0.5|       9.5|         0.0|                  1.0|       47.65|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-01 00:52:48|  2025-11-01 01:23:18|              1|         5.61|         1|                 N|          87|         255|           1|       33.1|  1.0|    0.5|       0.0|         0.0|                  1.0|       38.85|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-11-01 00:05:53|  2025-11-01 00:58:03|              1|          3.9|         1|                 N|         231|          43|           1|       40.8| 4.25|    0.5|       0.0|         0.0|                  1.0|       46.55|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-01 00:13:44|  2025-11-01 00:19:30|              2|         1.14|         1|                 N|         141|         262|           1|        7.9|  1.0|    0.5|       2.0|         0.0|                  1.0|        14.9|                 2.5|        0.0|               0.0|\n",
      "|       1| 2025-11-01 00:03:18|  2025-11-01 00:06:48|              2|          0.6|         1|                 N|         238|          24|           1|        5.1|  1.0|    0.5|      1.52|         0.0|                  1.0|        9.12|                 0.0|        0.0|               0.0|\n",
      "|       1| 2025-11-01 00:19:55|  2025-11-01 00:45:37|              1|          4.3|         1|                 N|         236|         147|           1|       24.7|  1.0|    0.5|       2.0|         0.0|                  1.0|        29.2|                 0.0|        0.0|               0.0|\n",
      "|       1| 2025-11-01 00:45:55|  2025-11-01 01:11:30|              1|          3.0|         1|                 N|         231|         137|           1|       24.0| 4.25|    0.5|       3.0|         0.0|                  1.0|       32.75|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-01 00:11:12|  2025-11-01 00:15:02|              1|         0.69|         1|                 N|         237|         237|           2|        6.5|  1.0|    0.5|       0.0|         0.0|                  1.0|        11.5|                 2.5|        0.0|               0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_yellow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eb68ce9-2497-4ad7-b8a0-dcf2e8ca0921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Repartition the DataFrame to 4 partitions and save it as Parquet files to a specified directory\n",
    "df = df_yellow \\\n",
    "          .repartition(4) \\\n",
    "          .write.mode(\"overwrite\").parquet('yellow/2025/11/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f0eaa-c73d-4b37-9eb8-bafc33fa571d",
   "metadata": {},
   "source": [
    "**Analysis of November 15th Taxi Trips**\n",
    "\n",
    "This section focuses on counting the total number of taxi trips that commenced on the 15th of November, considering only trips with a pickup date within that specific day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd670b08-2db1-49a5-a76d-a1ff562e8b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|trips_count|\n",
      "+-----------+\n",
      "|     162604|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary SQL view from the DataFrame\n",
    "df_yellow.createOrReplaceTempView(\"yellow_tripdata_2025_11\")\n",
    "\n",
    "# Execute a Spark SQL query to count trips on November 15th, 2025\n",
    "df_result = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*) AS trips_count\n",
    "FROM\n",
    "    `yellow_tripdata_2025_11`\n",
    "WHERE\n",
    "    tpep_pickup_datetime >= \"2025-11-15 00:00:00\"\n",
    "    AND\n",
    "    tpep_pickup_datetime < \"2025-11-16 00:00:00\"\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57c7ccf6-3c65-4df7-baed-942fd6c35203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of taxi trips on November 15th (PySpark API): 162604\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame to include only trips on November 15th, 2025 using PySpark API\n",
    "df_nov_15_pyspark = df_yellow.filter(\n",
    "    (col(\"tpep_pickup_datetime\") >= \"2025-11-15 00:00:00\") &\n",
    "    (col(\"tpep_pickup_datetime\") < \"2025-11-16 00:00:00\")\n",
    ")\n",
    "\n",
    "# Count the number of trips on October 15th\n",
    "trips_count_pyspark = df_nov_15_pyspark.count()\n",
    "\n",
    "# Print the count of trips\n",
    "print(f\"Number of taxi trips on November 15th (PySpark API): {trips_count_pyspark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793058b6-5f62-404a-93d4-c2cadca62f7a",
   "metadata": {},
   "source": [
    "**Analysis of the Longest Trip Duration**\n",
    "This section identifies the maximum trip length recorded within the dataset, measuring the duration in hours to determine which trip represents the longest continuous travel time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a186ca38-7464-48e1-bca1-404b35c0f67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|longest_trip_hours|\n",
      "+------------------+\n",
      "| 90.64666666666666|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute a Spark SQL query to find the longest trip duration in hours\n",
    "df_result = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    MAX((UNIX_TIMESTAMP(tpep_dropoff_datetime) - UNIX_TIMESTAMP(tpep_pickup_datetime)) / 3600) AS longest_trip_hours\n",
    "FROM\n",
    "    `yellow_tripdata_2025_11`\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecb671a2-29df-40c6-8cce-b0ba9bd56ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the longest trip in the dataset is: 90.65 hours\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, max\n",
    "\n",
    "# Calculate trip duration in seconds by subtracting pickup timestamp from dropoff timestamp\n",
    "df_with_duration = df_yellow.withColumn(\n",
    "    \"duration_seconds\",\n",
    "    unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))\n",
    ")\n",
    "\n",
    "# Convert duration from seconds to hours\n",
    "df_with_duration_hours = df_with_duration.withColumn(\n",
    "    \"duration_hours\",\n",
    "    col(\"duration_seconds\") / 3600\n",
    ")\n",
    "\n",
    "# Find the maximum duration in hours from the DataFrame\n",
    "longest_trip_hours = df_with_duration_hours.select(max(\"duration_hours\")).collect()[0][0]\n",
    "\n",
    "# Print the length of the longest trip, formatted to two decimal places\n",
    "print(f\"The length of the longest trip in the dataset is: {longest_trip_hours:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff05a25-7d0e-4c58-a743-cc7142da3c13",
   "metadata": {},
   "source": [
    "**Analysis of Spark Application Interface**\n",
    "This section focuses on identifying the local port used by Spark’s built‑in User Interface, which provides the application dashboard for monitoring job execution and cluster activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba492ba6-e1ad-4fd5-b044-bacaf2f0c710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI is running on: http://10.255.255.254:4040\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the Spark UI port from the SparkSession configuration\n",
    "spark_ui_url = spark.sparkContext.uiWebUrl\n",
    "print(f\"Spark UI is running on: {spark_ui_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c2a70c-28c2-4ba4-9375-7d3c69d11d15",
   "metadata": {},
   "source": [
    "**Analysis of Least Frequent Pickup Zone**\n",
    "This section focuses on identifying the pickup location zone with the lowest trip frequency. Using the zone lookup data—loaded into a temporary Spark view—and the Yellow Taxi trip data from October 2024, the objective is to determine which zone name appears least often as a pickup location in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e35f82b-271d-4046-a551-8f5857c8aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Download the taxi zone lookup CSV file\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv -O taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b267f4f-f38e-4777-aaa9-761376e37eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the taxi zone lookup CSV file into a Spark DataFrame, inferring schema and assuming header\n",
    "df_lookup = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ad6961a-b977-4cae-9498-8494495e40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary SQL view named 'taxi_zone_lookup' from the DataFrame 'df_lookup'\n",
    "df_lookup.createOrReplaceTempView(\"taxi_zone_lookup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74196d78-b432-4733-a315-b8bc3f7368b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                Zone|trip_count|\n",
      "+--------------------+----------+\n",
      "|Governor's Island...|         1|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result_sql = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    tz.Zone,\n",
    "    COUNT(yt.PULocationID) AS trip_count\n",
    "FROM\n",
    "    `yellow_tripdata_2025_11` yt\n",
    "JOIN\n",
    "    `taxi_zone_lookup` tz ON yt.PULocationID = tz.LocationID\n",
    "GROUP BY\n",
    "    tz.Zone\n",
    "ORDER BY\n",
    "    trip_count ASC\n",
    "LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b2786b6-5f89-4ea0-accd-f0a0066e9eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LEAST frequent pickup location Zone is: Governor's Island/Ellis Island/Liberty Island with 1 trips.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Join the yellow trip data DataFrame with the zone lookup DataFrame on PULocationID and LocationID\n",
    "df_joined = df_yellow.join(\n",
    "    df_lookup,\n",
    "    df_yellow.PULocationID == df_lookup.LocationID,\n",
    "    \"left\" # Use a left join to keep all records from the yellow trip data\n",
    ")\n",
    "\n",
    "# Group the joined DataFrame by 'Zone' and count the number of trips (PULocationID) in each zone\n",
    "df_zone_counts = df_joined.groupBy(\"Zone\").agg(count(\"PULocationID\").alias(\"trip_count\"))\n",
    "\n",
    "# Order the zones by trip count in ascending order and get the first row (least frequent zone)\n",
    "least_frequent_zone = df_zone_counts.orderBy(col(\"trip_count\")).first()\n",
    "\n",
    "# Check if a least frequent zone was found and print the result\n",
    "if least_frequent_zone:\n",
    "    print(f\"The LEAST frequent pickup location Zone is: {least_frequent_zone['Zone']} with {least_frequent_zone['trip_count']} trips.\")\n",
    "else:\n",
    "    print(\"Could not determine the least frequent pickup location zone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "06-batch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
