{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0e21c-59ff-4d9f-af80-56fddb160c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pyspark\n",
    "form pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6947a1-f919-4569-a202-f2de6e2c44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('nyc-taxi') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10aa3b-0c1b-441d-b425-bafeff783a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pysprk version\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f2730-f7e8-446e-82f4-0dc9e062f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Download the Parquet file for Yellow Taxi Trip Data from October 2024\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-11.parquet -O yellow_tripdata_2025-11.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec87ef5c-be5a-4cf7-a8e3-aa75c982cf4b",
   "metadata": {},
   "source": [
    "**Analysis of Partitioned Parquet File Sizes**\n",
    "This section examines the average size (in megabytes) of all files generated with the .parquet extension, focusing on determining which value most closely represents the typical file size among those created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c78f8-99e6-4b95-8ef8-ad0fa04a7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the downloaded Parquet file into a Spark DataFrame\n",
    "df_yellow = spark.read.parquet('yellow_tripdata_2025-11.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ee6d6-6bb1-4f28-a55d-f190cc449407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb68ce9-2497-4ad7-b8a0-dcf2e8ca0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition the DataFrame to 4 partitions and save it as Parquet files to a specified directory\n",
    "df = df_yellow \\\n",
    "          .repartition(4) \\\n",
    "          .write.mode(\"overwrite\").parquet('yellow/2025/11/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f0eaa-c73d-4b37-9eb8-bafc33fa571d",
   "metadata": {},
   "source": [
    "**Analysis of November 15th Taxi Trips**\n",
    "\n",
    "This section focuses on counting the total number of taxi trips that commenced on the 15th of November, considering only trips with a pickup date within that specific day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd670b08-2db1-49a5-a76d-a1ff562e8b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary SQL view from the DataFrame\n",
    "df_yellow.createOrReplaceTempView(\"yellow_tripdata_2025_11\")\n",
    "\n",
    "# Execute a Spark SQL query to count trips on November 15th, 2025\n",
    "df_result = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*) AS trips_count\n",
    "FROM\n",
    "    `yellow_tripdata_2025_11`\n",
    "WHERE\n",
    "    tpep_pickup_datetime >= \"2025-11-15 00:00:00\"\n",
    "    AND\n",
    "    tpep_pickup_datetime < \"2025-11-16 00:00:00\"\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7ccf6-3c65-4df7-baed-942fd6c35203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame to include only trips on November 15th, 2025 using PySpark API\n",
    "df_nov_15_pyspark = df_yellow.filter(\n",
    "    (col(\"tpep_pickup_datetime\") >= \"2025-11-15 00:00:00\") &\n",
    "    (col(\"tpep_pickup_datetime\") < \"2025-11-16 00:00:00\")\n",
    ")\n",
    "\n",
    "# Count the number of trips on October 15th\n",
    "trips_count_pyspark = df_nov_15_pyspark.count()\n",
    "\n",
    "# Print the count of trips\n",
    "print(f\"Number of taxi trips on November 15th (PySpark API): {trips_count_pyspark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793058b6-5f62-404a-93d4-c2cadca62f7a",
   "metadata": {},
   "source": [
    "**Analysis of the Longest Trip Duration**\n",
    "This section identifies the maximum trip length recorded within the dataset, measuring the duration in hours to determine which trip represents the longest continuous travel time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186ca38-7464-48e1-bca1-404b35c0f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute a Spark SQL query to find the longest trip duration in hours\n",
    "df_result = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    MAX((UNIX_TIMESTAMP(tpep_dropoff_datetime) - UNIX_TIMESTAMP(tpep_pickup_datetime)) / 3600) AS longest_trip_hours\n",
    "FROM\n",
    "    `yellow_tripdata_2025_11`\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb671a2-29df-40c6-8cce-b0ba9bd56ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, max\n",
    "\n",
    "# Calculate trip duration in seconds by subtracting pickup timestamp from dropoff timestamp\n",
    "df_with_duration = df_yellow.withColumn(\n",
    "    \"duration_seconds\",\n",
    "    unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))\n",
    ")\n",
    "\n",
    "# Convert duration from seconds to hours\n",
    "df_with_duration_hours = df_with_duration.withColumn(\n",
    "    \"duration_hours\",\n",
    "    col(\"duration_seconds\") / 3600\n",
    ")\n",
    "\n",
    "# Find the maximum duration in hours from the DataFrame\n",
    "longest_trip_hours = df_with_duration_hours.select(max(\"duration_hours\")).collect()[0][0]\n",
    "\n",
    "# Print the length of the longest trip, formatted to two decimal places\n",
    "print(f\"The length of the longest trip in the dataset is: {longest_trip_hours:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff05a25-7d0e-4c58-a743-cc7142da3c13",
   "metadata": {},
   "source": [
    "**Analysis of Spark Application Interface**\n",
    "This section focuses on identifying the local port used by Spark’s built‑in User Interface, which provides the application dashboard for monitoring job execution and cluster activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba492ba6-e1ad-4fd5-b044-bacaf2f0c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the Spark UI port from the SparkSession configuration\n",
    "spark_ui_url = spark.sparkContext.uiWebUrl\n",
    "print(f\"Spark UI is running on: {spark_ui_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c2a70c-28c2-4ba4-9375-7d3c69d11d15",
   "metadata": {},
   "source": [
    "**Analysis of Least Frequent Pickup Zone**\n",
    "This section focuses on identifying the pickup location zone with the lowest trip frequency. Using the zone lookup data—loaded into a temporary Spark view—and the Yellow Taxi trip data from October 2024, the objective is to determine which zone name appears least often as a pickup location in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e35f82b-271d-4046-a551-8f5857c8aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Download the taxi zone lookup CSV file\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv -O taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b267f4f-f38e-4777-aaa9-761376e37eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the taxi zone lookup CSV file into a Spark DataFrame, inferring schema and assuming header\n",
    "df_lookup = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6961a-b977-4cae-9498-8494495e40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary SQL view named 'taxi_zone_lookup' from the DataFrame 'df_lookup'\n",
    "df_lookup.createOrReplaceTempView(\"taxi_zone_lookup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74196d78-b432-4733-a315-b8bc3f7368b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_sql = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    tz.Zone,\n",
    "    COUNT(yt.PULocationID) AS trip_count\n",
    "FROM\n",
    "    `yellow_tripdata_2025_11` yt\n",
    "JOIN\n",
    "    `taxi_zone_lookup` tz ON yt.PULocationID = tz.LocationID\n",
    "GROUP BY\n",
    "    tz.Zone\n",
    "ORDER BY\n",
    "    trip_count ASC\n",
    "LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2786b6-5f89-4ea0-accd-f0a0066e9eba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_yellow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, count\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Join the yellow trip data DataFrame with the zone lookup DataFrame on PULocationID and LocationID\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df_joined = \u001b[43mdf_yellow\u001b[49m.join(\n\u001b[32m      5\u001b[39m     df_lookup,\n\u001b[32m      6\u001b[39m     df_yellow.PULocationID == df_lookup.LocationID,\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# Use a left join to keep all records from the yellow trip data\u001b[39;00m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Group the joined DataFrame by 'Zone' and count the number of trips (PULocationID) in each zone\u001b[39;00m\n\u001b[32m     11\u001b[39m df_zone_counts = df_joined.groupBy(\u001b[33m\"\u001b[39m\u001b[33mZone\u001b[39m\u001b[33m\"\u001b[39m).agg(count(\u001b[33m\"\u001b[39m\u001b[33mPULocationID\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mtrip_count\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'df_yellow' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Join the yellow trip data DataFrame with the zone lookup DataFrame on PULocationID and LocationID\n",
    "df_joined = df_yellow.join(\n",
    "    df_lookup,\n",
    "    df_yellow.PULocationID == df_lookup.LocationID,\n",
    "    \"left\" # Use a left join to keep all records from the yellow trip data\n",
    ")\n",
    "\n",
    "# Group the joined DataFrame by 'Zone' and count the number of trips (PULocationID) in each zone\n",
    "df_zone_counts = df_joined.groupBy(\"Zone\").agg(count(\"PULocationID\").alias(\"trip_count\"))\n",
    "\n",
    "# Order the zones by trip count in ascending order and get the first row (least frequent zone)\n",
    "least_frequent_zone = df_zone_counts.orderBy(col(\"trip_count\")).first()\n",
    "\n",
    "# Check if a least frequent zone was found and print the result\n",
    "if least_frequent_zone:\n",
    "    print(f\"The LEAST frequent pickup location Zone is: {least_frequent_zone['Zone']} with {least_frequent_zone['trip_count']} trips.\")\n",
    "else:\n",
    "    print(\"Could not determine the least frequent pickup location zone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1202559a-b80e-4e6f-9194-4f2307d3a5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
